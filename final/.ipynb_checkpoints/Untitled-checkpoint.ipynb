{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open csv, drop unnecessary columns\n",
    "nba = pd.read_csv(\"../nba.csv\")\n",
    "nba = nba.drop(columns = [\"Unnamed: 6\", \"Start (ET)\",\"Notes\",\"Unnamed: 7\",\"Attend.\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean date and turn to date_time type\n",
    "nba.Date = nba.Date.str.replace(\"^[A-z]{3}\",\"-\")\n",
    "nba.Date.str.lstrip(\"- \")\n",
    "nba.Date = nba.Date.str.replace(\" \",\"-\").str.lstrip(\"-\")\n",
    "nba.Date = pd.to_datetime(nba.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns\n",
    "nba = nba.rename(columns = {\"PTS\":\"AwayPTS\", \"PTS.1\":\"HomePTS\", \"Visitor/Neutral\":\"Away\",\"Home/Neutral\":\"Home\", \"Attend.\":\"Attend\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating homewins columns\n",
    "nba[\"HomeWin\"] = np.where(nba[\"HomePTS\"] > nba[\"AwayPTS\"], 1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeawaytest = nba.drop(columns = [\"Date\",\"AwayPTS\",\"HomePTS\"])\n",
    "timeawaytest[\"TimeAway\"] = nbatest[\"Timeaway\"]\n",
    "timeawaytest[\"HomeWinStreak\"] = nbatest[\"HomeWinStreak\"]\n",
    "timeawaytest[[\"AwayWinStreak\",\"HomeLoseStreak\",\"AwayLoseStreak\",\"HomeCoachSavage\",\"AwayCoachSavage\"]]=nbatest[[\"AwayWinStreak\",\"HomeLoseStreak\",\"AwayLoseStreak\",\"HomeCoachSavage\",\"AwayCoachSavage\"]]\n",
    "timeawaytest[[\"HomeAllStars\",\"AwayAllStars\"]] = nbatest[[\"HomeAllStars\",\"AwayAllStars\"]]\n",
    "\n",
    "timeawaytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatest = nba[[\"Away\",\"Home\",\"HomeWin\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py:3489: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "dummies = pd.get_dummies(nbatest[[\"Away\",\"Home\"]])\n",
    "nbatest[dummies.columns] = dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatest = nbatest.drop(columns = [\"Home\",\"Away\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining functions that check the accuracy score of our predictions using different models\n",
    "#Random Forests\n",
    "def RFscore(nbatest):\n",
    "    X_train = nbatest[:984].drop(columns = \"HomeWin\")    \n",
    "    y_train = nbatest[\"HomeWin\"][:984]\n",
    "    X_test = nbatest[984:].drop(columns = \"HomeWin\")\n",
    "    y_test = nbatest[\"HomeWin\"][984:]\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.datasets import make_classification\n",
    "    mod = RandomForestClassifier(n_estimators = 100)\n",
    "    mod.fit(X_train, y_train)\n",
    "    y_pred = mod.predict(X_test)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(y_test,y_pred)\n",
    "\n",
    "\n",
    "#SVC\n",
    "def SVCscore(nbatest):\n",
    "    #train data/ test data\n",
    "    X_train = nbatest[:984].drop(columns = \"HomeWin\")    \n",
    "    y_train = nbatest[\"HomeWin\"][:984]\n",
    "    X_test = nbatest[984:].drop(columns = \"HomeWin\")\n",
    "    y_test = nbatest[\"HomeWin\"][984:]\n",
    "    \n",
    "    #fit model\n",
    "    from sklearn.svm import SVC\n",
    "    svc = SVC(gamma='auto')\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(y_test,y_pred)\n",
    "\n",
    "\n",
    "#LogisticRegression\n",
    "def LRscore(nbatest):\n",
    "    X_train = nbatest[:984].drop(columns = \"HomeWin\")    \n",
    "    y_train = nbatest[\"HomeWin\"][:984]\n",
    "    X_test = nbatest[984:].drop(columns = \"HomeWin\")\n",
    "    y_test = nbatest[\"HomeWin\"][984:]\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    mod = LogisticRegression()\n",
    "    mod.fit(X_train,y_train)\n",
    "    y_pred = mod.predict(X_test)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score:  0.6747967479674797 \n",
      "SVC Accuracy score: 0.5934959349593496 \n",
      "Random Forests Accuracy score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score:  0.6788617886178862 \n",
      "SVC Accuracy score: 0.5934959349593496 \n",
      "Random Forests Accuracy score: 0.6219512195121951\n"
     ]
    }
   ],
   "source": [
    "#Adding \"time on the road\" feature\n",
    "nbatest[\"Timeaway\"] = 0\n",
    "away_counts = {}\n",
    "for i in nba[\"Away\"].unique():\n",
    "    away_counts[i] = 0\n",
    "for i in range(len(nba.Away)):\n",
    "    away_counts[nba.Away[i]] +=1\n",
    "    away_counts[nba.Home[i]] = 0\n",
    "    nbatest.iloc[i,-1] = away_counts[nba.Away[i]]\n",
    "\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score:  0.6788617886178862 \n",
      "SVC Accuracy score: 0.5934959349593496 \n",
      "Random Forests Accuracy score: 0.6504065040650406\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(nbatest[[\"Timeaway\"]])\n",
    "scaled = scaler.transform(nbatest[[\"Timeaway\"]])\n",
    "nbatest[[\"Timeaway\"]]= scaled\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score:  0.6991869918699187 \n",
      "SVC Accuracy score: 0.6747967479674797 \n",
      "Random Forests Accuracy score: 0.6951219512195121\n"
     ]
    }
   ],
   "source": [
    "#adding Home win streak and Away win streak features\n",
    "nbatest[\"HomeWinStreak\"] = 0\n",
    "nbatest[\"AwayWinStreak\"] = 0\n",
    "win_counts = {}\n",
    "for i in nba[\"Away\"].unique():\n",
    "    win_counts[i] = 0\n",
    "    \n",
    "for row in range(len(nba)):\n",
    "    if nbatest[\"HomeWin\"][row] == 1:\n",
    "        win_counts[nba[\"Home\"][row]] +=1\n",
    "        win_counts[nba[\"Away\"][row]] == 0\n",
    "    else:\n",
    "        win_counts[nba[\"Away\"][row]] +=1\n",
    "        win_counts[nba[\"Home\"][row]] == 0\n",
    "    nbatest.iloc[row,-2] = win_counts[nba[\"Home\"][row]]\n",
    "    nbatest.iloc[row,-1] = win_counts[nba[\"Away\"][row]]\n",
    "    \n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score:  0.6991869918699187 \n",
      "SVC Accuracy score: 0.7154471544715447 \n",
      "Random Forests Accuracy score: 0.6829268292682927\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(nbatest[[\"HomeWinStreak\",\"AwayWinStreak\"]])\n",
    "scaled = scaler.transform(nbatest[[\"HomeWinStreak\",\"AwayWinStreak\"]])\n",
    "nbatest[[\"HomeWinStreak\",\"AwayWinStreak\"]]= scaled\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVCrandscore(nbatest):\n",
    "    #train data/ test data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = nbatest.drop(columns = \"HomeWin\")\n",
    "    y = nbatest[\"HomeWin\"]\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=.8)\n",
    "    \n",
    "    #fit model\n",
    "    from sklearn.svm import SVC\n",
    "    svc = SVC(gamma='auto')\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score:  0.6991869918699187 \n",
      "SVC Accuracy score: 0.6422764227642277 \n",
      "Random Forests Accuracy score: 0.5975609756097561\n"
     ]
    }
   ],
   "source": [
    "#adding Home lose streak and Away lose streak features\n",
    "nbatest[\"HomeLoseStreak\"] = 0\n",
    "nbatest[\"AwayLoseStreak\"] = 0\n",
    "loss_counts = {}\n",
    "for i in nba[\"Away\"].unique():\n",
    "    loss_counts[i] = 0\n",
    "    \n",
    "\n",
    "for row in range(len(nba)):\n",
    "    if nbatest[\"HomeWin\"][row] == 1:\n",
    "        loss_counts[nba[\"Away\"][row]] +=1\n",
    "        loss_counts[nba[\"Home\"][row]] == 0\n",
    "    else:\n",
    "        loss_counts[nba[\"Home\"][row]] +=1\n",
    "        loss_counts[nba[\"Away\"][row]] == 0\n",
    "    nbatest.iloc[row,-2] = loss_counts[nba[\"Home\"][row]]\n",
    "    nbatest.iloc[row,-1] = loss_counts[nba[\"Away\"][row]]\n",
    "    \n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score:  0.6991869918699187 \n",
      "SVC Accuracy score: 0.7032520325203252 \n",
      "Random Forests Accuracy score: 0.7154471544715447\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(nbatest[[\"HomeLoseStreak\",\"AwayLoseStreak\"]])\n",
    "scaled = scaler.transform(nbatest[[\"HomeLoseStreak\",\"AwayLoseStreak\"]])\n",
    "nbatest[[\"HomeLoseStreak\",\"AwayLoseStreak\"]]= scaled\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score:  0.6951219512195121 \n",
      "SVC Accuracy score: 0.7113821138211383 \n",
      "Random Forests Accuracy score: 0.6097560975609756\n"
     ]
    }
   ],
   "source": [
    "nbatest[\"HomeCoachSavage\"] = 0\n",
    "nbatest[\"AwayCoachSavage\"] = 0\n",
    "\n",
    "nbatest.loc[nba[\"Home\"]==\"San Antonio Spurs\",\"HomeCoachSavage\"] = 6\n",
    "nbatest.loc[nba[\"Home\"]==\"Boston Celtics\",\"HomeCoachSavage\"] = 5\n",
    "nbatest.loc[nba[\"Home\"]==\"Golden State Warriors\",\"HomeCoachSavage\"] = 4\n",
    "nbatest.loc[nba[\"Home\"]==\"Utah Jazz\",\"HomeCoachSavage\"] = 3\n",
    "nbatest.loc[nba[\"Home\"]==\"Houston Rockets\",\"HomeCoachSavage\"] = 2\n",
    "nbatest.loc[nba[\"Home\"]==\"Toronto Raptors\",\"HomeCoachSavage\"] = 1\n",
    "\n",
    "\n",
    "\n",
    "nbatest.loc[nba[\"Away\"]==\"San Antonio Spurs\",\"AwayCoachSavage\"] = 6\n",
    "nbatest.loc[nba[\"Away\"]==\"Boston Celtics\",\"AwayCoachSavage\"] = 5\n",
    "nbatest.loc[nba[\"Away\"]==\"Golden State Warriors\",\"AwayCoachSavage\"] = 4\n",
    "nbatest.loc[nba[\"Away\"]==\"Utah Jazz\",\"AwayCoachSavage\"] = 3\n",
    "nbatest.loc[nba[\"Away\"]==\"Houston Rockets\",\"AwayCoachSavage\"] = 2\n",
    "nbatest.loc[nba[\"Away\"]==\"Toronto Raptors\",\"AwayCoachSavage\"] = 1\n",
    "\n",
    "\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score:  0.6951219512195121 \n",
      "SVC Accuracy score: 0.7113821138211383 \n",
      "Random Forests Accuracy score: 0.6707317073170732\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(nbatest[[\"HomeCoachSavage\",\"AwayCoachSavage\"]])\n",
    "scaled = scaler.transform(nbatest[[\"HomeCoachSavage\",\"AwayCoachSavage\"]])\n",
    "nbatest[[\"HomeCoachSavage\",\"AwayCoachSavage\"]]= scaled\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allstar_count = {}\n",
    "for i in nba.Home.unique():\n",
    "    allstar_count[i] = 0\n",
    "allstar_count['Philadelphia 76ers'] = 2\n",
    "allstar_count['Milwaukee Bucks'] = 2\n",
    "allstar_count['Oklahoma City Thunder']=2\n",
    "allstar_count['Golden State Warriors'] = 3\n",
    "allstar_count['Denver Nuggets'] = 1\n",
    "allstar_count['Detroit Pistons'] = 1\n",
    "allstar_count['Brooklyn Nets'] = 1\n",
    "allstar_count['Orlando Magic'] = 1\n",
    "allstar_count['Toronto Raptors'] = 2\n",
    "allstar_count['Dallas Mavericks'] = 1\n",
    "allstar_count['Los Angeles Lakers'] = 1\n",
    "allstar_count['Houston Rockets'] = 1\n",
    "allstar_count['Orlando Magic'] = 1\n",
    "allstar_count['Boston Celtics'] = 1\n",
    "allstar_count['New Orleans Pelicans'] = 1\n",
    "allstar_count[ 'Portland Trail Blazers'] = 1\n",
    "allstar_count['San Antonio Spurs'] = 1\n",
    "allstar_count['Minnesota Timberwolves'] = 1\n",
    "allstar_count['Washington Wizards'] = 1\n",
    "allstar_count['Miami Heat'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatest[\"HomeAllstars\"] = 0\n",
    "nbatest[\"AwayAllstars\"] = 0\n",
    "for i in range(len(nba)):\n",
    "    nbatest.iloc[i,-1] = allstar_count[nba[\"Away\"][i]]\n",
    "    nbatest.iloc[i,-2] = allstar_count[nba[\"Home\"][i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(nbatest[[\"HomeAllstars\",\"AwayAllstars\"]])\n",
    "scaled = scaler.transform(nbatest[[\"HomeAllstars\",\"AwayAllstars\"]])\n",
    "nbatest[[\"HomeAllstars\",\"AwayAllstars\"]]= scaled\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "depth = pd.read_csv(\"depth.csv\")\n",
    "depth[\"avg\"] = (depth[\"bench1\"]+depth[\"bench2\"]+depth[\"bench3\"]+depth[\"bench4\"]+depth[\"bench5\"])/5\n",
    "\n",
    "# depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_dict ={}\n",
    "for i in depth.team.unique():\n",
    "    depth_dict[i] = float(depth[depth[\"team\"] == i][\"avg\"])\n",
    "nba.head()\n",
    "print(depth_dict, nba.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatest[\"HomeBenchRating\"] = 0\n",
    "nbatest[\"AwayBenchRating\"] = 0\n",
    "for i in range(len(nba)):\n",
    "    nbatest.iloc[i,-1] = depth_dict[nba.Away[i]]\n",
    "    nbatest.iloc[i,-2] = depth_dict[nba.Home[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(nbatest[[\"AwayBenchRating\",\"HomeBenchRating\"]])\n",
    "scaled = scaler.transform(nbatest[[\"AwayBenchRating\",\"HomeBenchRating\"]])\n",
    "nbatest[[\"AwayBenchRating\",\"HomeBenchRating\"]]= scaled\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "for i in nba.Home.unique():\n",
    "    score_dict[i] = {\"counter\" : 0, \"totalscore\" : 0}\n",
    "score_dict[\"Miami Heat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatest[\"AwayAvgScore\"] = 0\n",
    "nbatest[\"HomeAvgScore\"] = 0\n",
    "for i in range(len(nba)):\n",
    "    #Home team counter and total score\n",
    "    score_dict[nba.Home[i]][\"counter\"] +=1 \n",
    "    score_dict[nba.Home[i]][\"totalscore\"] += nba.HomePTS[i]\n",
    "    #Away team counter and score\n",
    "    score_dict[nba.Away[i]][\"counter\"]+= 1 \n",
    "    score_dict[nba.Away[i]][\"totalscore\"] += nba.AwayPTS[i]\n",
    "    \n",
    "    nbatest.iloc[i,-1] = score_dict[nba.Home[i]][\"totalscore\"] / score_dict[nba.Home[i]][\"counter\"]\n",
    "    nbatest.iloc[i,-2] = score_dict[nba.Away[i]][\"totalscore\"] / score_dict[nba.Away[i]][\"counter\"]\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(nbatest[[\"AwayAvgScore\",\"HomeAvgScore\"]])\n",
    "scaled = scaler.transform(nbatest[[\"AwayAvgScore\",\"HomeAvgScore\"]])\n",
    "nbatest[[\"AwayAvgScore\",\"HomeAvgScore\"]]= scaled\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(nbatest[['Timeaway', 'HomeWinStreak', 'AwayWinStreak',\n",
    "       'HomeLoseStreak', 'AwayLoseStreak', 'HomeCoachSavage',\n",
    "       'AwayCoachSavage', 'HomeAllstars', 'AwayAllstars',\"HomeBenchRating\",\"AwayBenchRating\"]].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANKINGS\n",
    "#Creates dictionary with team and their rankings\n",
    "feb_ranks = pd.read_csv(\"feb_ranks.csv\")\n",
    "feb_ranks[\"Team\"] = feb_ranks[\"Western Conference\"]\n",
    "rankdict = {}\n",
    "for i in range(len(feb_ranks.Team)):\n",
    "    rankdict[feb_ranks.Team[i]] = feb_ranks.Rk[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates HomeRank and AwayRank for each matchup in nba dataset\n",
    "nbatest[\"HomeRank\"] = 0\n",
    "nbatest[\"AwayRank\"] = 0\n",
    "for i in range(len(nbatest.HomeRank)):\n",
    "    #Setting AwayRank for row i\n",
    "    nbatest.iloc[i,-1] = rankdict[nba.Away[i]]\n",
    "    #Setting HomeRank for row i\n",
    "    nbatest.iloc[i,-2] = rankdict[nba.Home[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(nbatest[[\"HomeRank\",\"AwayRank\"]])\n",
    "scaled = scaler.transform(nbatest[[\"HomeRank\",\"AwayRank\"]])\n",
    "nbatest[[\"HomeRank\",\"AwayRank\"]]= scaled\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a single column that checks if the home team is ranked higher than away team \n",
    "rankspread = nbatest[\"HomeRank\"]-nbatest[\"AwayRank\"]\n",
    "nbatest[\"HomeRanksHigher\"]= 0\n",
    "#if rankspread is positive, home is better\n",
    "#if rankspread is negative, away is better\n",
    "for i in range(len(nbatest)):\n",
    "    if rankspread[i] > 0:\n",
    "        nbatest.iloc[i,-1] = 1\n",
    "    else:\n",
    "        nbatest.iloc[i,-1] = 0\n",
    "        \n",
    "\n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates  a single column that checks if the home team is significantly better than the away team\n",
    "rankspread = nbatest[\"HomeRank\"]-nbatest[\"AwayRank\"]\n",
    "\n",
    "nbatest[\"HomeisFav\"]= 0\n",
    "for i in range(len(nbatest)):\n",
    "    if rankspread[i] > 0 and rankspread[i] >5:\n",
    "        nbatest.iloc[i,-1] = 1\n",
    "    elif rankspread[i]<0 and abs(rankspread[i])>5:\n",
    "        nbatest.iloc[i,-1] = 0\n",
    "    else:\n",
    "        nbatest.iloc[i,-1] = 0\n",
    "nbatest.drop(columns = [\"HomeRank\",\"AwayRank\"], inplace = True) \n",
    "print(\"Logistic Regression Accuracy Score: \",LRscore(nbatest),\"\\nSVC Accuracy score:\",SVCscore(nbatest),\"\\nRandom Forests Accuracy score:\",RFscore(nbatest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nbatest[[\"HomeWinStreak\",\"AwayWinStreak\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
